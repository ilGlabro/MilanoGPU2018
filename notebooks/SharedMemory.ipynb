{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world\n"
     ]
    }
   ],
   "source": [
    "print('Hello world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "\n",
    "import pycuda.driver as cuda_driver\n",
    "import pycuda.compiler as cuda_compiler\n",
    "from pycuda.gpuarray import GPUArray\n",
    "\n",
    "import IPythonMagic\n",
    "from Timer import Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global logger already initialized!\n"
     ]
    }
   ],
   "source": [
    "%setup_logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registering context in user workspace\n",
      "Context already registered! Ignoring\n"
     ]
    }
   ],
   "source": [
    "%cuda_context_handler context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.6/site-packages/ipykernel_launcher.py:66: UserWarning: The CUDA compiler succeeded, but said the following:\n",
      "kernel.cu(7): warning: variable \"gid\" was declared but never referenced\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kernel_src = \"\"\"\n",
    "\n",
    "__global__ void shmemReduction(float* output, float* input, int size) {\n",
    "    //First we stride through global memory and compute\n",
    "    //the maximum for every thread\n",
    "    int gid = blockIdx.x * blockDim.x + threadIdx.x; //blockIdx is always 0 because we use just 1 block\n",
    "    \n",
    "    float max_value = -9999999.99;    \n",
    "    for (int i = threadIdx.x; i < size; i = i + blockDim.x) {\n",
    "        max_value = fmaxf(max_value, input[i]);\n",
    "    }\n",
    "    \n",
    "    //Temporary write to memory to check if things work so far\n",
    "    output[threadIdx.x] = max_value;\n",
    "    \n",
    "    //Store the per-thread maximum in shared memory\n",
    "    __shared__ float max_shared[128];\n",
    "    max_shared[threadIdx.x] = max_value;\n",
    "        \n",
    "    //Synchronize so that all thread see the same shared memory\n",
    "    __syncthreads();\n",
    "    \n",
    "    //Find the maximum in shared memory\n",
    "    \n",
    "    //Reduce from 128 to 64\n",
    "     if (threadIdx.x < 64) {\n",
    "        max_shared[threadIdx.x] = fmaxf(max_shared[threadIdx.x], max_shared[threadIdx.x + 64]);\n",
    "    }\n",
    "    \n",
    "    //\n",
    "    //\n",
    "    __syncthreads();    \n",
    "    \n",
    "    //Reduce from 64 to 32\n",
    "     if (threadIdx.x < 32) {\n",
    "        max_shared[threadIdx.x] = fmaxf(max_shared[threadIdx.x], max_shared[threadIdx.x + 32]);\n",
    "    }\n",
    "    \n",
    "    //Reduce from 32 to 16\n",
    "     if (threadIdx.x < 16) {\n",
    "        max_shared[threadIdx.x] = fmaxf(max_shared[threadIdx.x], max_shared[threadIdx.x + 16]);\n",
    "    }\n",
    "    \n",
    "    //Reduce from 16 to 8\n",
    "     if (threadIdx.x < 8) {\n",
    "        max_shared[threadIdx.x] = fmaxf(max_shared[threadIdx.x], max_shared[threadIdx.x + 8]);\n",
    "    }\n",
    "    \n",
    "    //Reduce from 8 to 4\n",
    "     if (threadIdx.x < 4) {\n",
    "        max_shared[threadIdx.x] = fmaxf(max_shared[threadIdx.x], max_shared[threadIdx.x + 4]);\n",
    "    }\n",
    "    \n",
    "    //Reduce from 4 to 2\n",
    "     if (threadIdx.x < 2) {\n",
    "        max_shared[threadIdx.x] = fmaxf(max_shared[threadIdx.x], max_shared[threadIdx.x + 2]);\n",
    "    }\n",
    "    \n",
    "    //Reduce from 2 to 1\n",
    "     if (threadIdx.x < 1) {\n",
    "        max_shared[threadIdx.x] = fmaxf(max_shared[threadIdx.x], max_shared[threadIdx.x + 1]);\n",
    "    }\n",
    "    \n",
    "    //Finally write out to output\n",
    "    if (threadIdx.x == 0) {\n",
    "        output[0] = max_shared[0];    \n",
    "    }\n",
    "\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "kernel_module = cuda_compiler.SourceModule(kernel_src)\n",
    "kernel_function = kernel_module.get_function(\"shmemReduction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 128\n",
    "a = np.random.random((1,n)).astype(np.float32)\n",
    "\n",
    "a_g = GPUArray(a.shape, a.dtype)\n",
    "a_g.set(a)\n",
    "\n",
    "num_threads = 64\n",
    "b = np.empty((1,num_threads), dtype=np.float32)\n",
    "\n",
    "b_g = GPUArray(b.shape, b.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.73013583e-01 8.73201132e-01 7.15976775e-01 4.29855943e-01\n",
      "  7.98393726e-01 6.90453649e-01 4.65036966e-02 2.94540346e-01\n",
      "  9.56288040e-01 2.96723366e-01 8.00974071e-01 9.98879850e-01\n",
      "  7.92831182e-01 8.63308728e-01 6.15976274e-01 1.59680218e-01\n",
      "  3.05530250e-01 6.17582798e-01 5.25161922e-01 9.84551534e-02\n",
      "  9.36919093e-01 3.47840071e-01 5.93698800e-01 5.15059352e-01\n",
      "  2.80975908e-01 1.70232698e-01 4.40574974e-01 8.81036557e-03\n",
      "  9.07207072e-01 8.45832229e-01 4.25834179e-01 1.77027866e-01\n",
      "  2.78126299e-01 3.65826905e-01 1.28332987e-01 5.31588674e-01\n",
      "  8.48873734e-01 1.07728995e-01 2.94347793e-01 3.21659476e-01\n",
      "  5.69149137e-01 5.14385998e-01 6.74198789e-04 1.25035644e-01\n",
      "  4.53276306e-01 1.29071116e-01 4.30171847e-01 6.19124591e-01\n",
      "  2.83433765e-01 1.28407419e-01 9.46896896e-02 1.78152412e-01\n",
      "  2.22793013e-01 2.03935713e-01 2.90741563e-01 4.49092180e-01\n",
      "  2.20332116e-01 4.68699694e-01 1.12560056e-02 9.14939702e-01\n",
      "  7.22574472e-01 7.14568138e-01 5.39393663e-01 2.61559874e-01]]\n",
      "[[0.99887985 0.87320113 0.7159768  0.5315887  0.84887373 0.69045365\n",
      "  0.2943478  0.32165948 0.95628804 0.514386   0.8009741  0.99887985\n",
      "  0.7928312  0.8633087  0.6159763  0.6191246  0.30553025 0.6175828\n",
      "  0.5251619  0.17815241 0.9369191  0.34784007 0.5936988  0.51505935\n",
      "  0.2809759  0.4686997  0.44057497 0.9149397  0.9072071  0.8458322\n",
      "  0.53939366 0.26155987]]\n"
     ]
    }
   ],
   "source": [
    "block_size = (num_threads, 1, 1)\n",
    "grid_size = (1, 1, 1)\n",
    "\n",
    "kernel_function(b_g, a_g, np.int32(n), np.int32(n),grid=grid_size, block=block_size)\n",
    "\n",
    "b_g.get(b)\n",
    "\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
